{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "REINFORCE with Custom Reward Shaping\n",
        "Author: Darren Wu"
      ],
      "metadata": {
        "id": "v95qgYT8snyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import math\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "ecjIzJjFsw0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "GAMMA = 0.99\n",
        "LR = 1e-3\n",
        "NUM_EPISODES = 1000\n",
        "PRINT_EVERY = 50\n",
        "DEVICE = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "k6fe8OErs0jA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# monkey patch for deprecated numpy aliasing error.\n",
        "if not hasattr(np, \"bool8\"):\n",
        "    np.bool8 = np.bool_"
      ],
      "metadata": {
        "id": "EaIyZbSkXuJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reward weights\n",
        "ENABLE_ACTION_CHANGE_PENALTY = True\n",
        "ACTION_CHANGE_PENALTY_WEIGHT = 0.001\n",
        "\n",
        "ENABLE_STATE_DEPENDENT_COST = True\n",
        "STATE_DEPENDENT_COST_WEIGHT = 0.001\n",
        "\n",
        "ENABLE_EXPLORATION_BONUS = True\n",
        "EXPLORATION_BONUS_WEIGHT = 0.01\n",
        "EXPLORATION_GRID_SIZE = 0.1\n",
        "\n",
        "HIDDEN_SIZE = 64"
      ],
      "metadata": {
        "id": "8IhLOTNWs3RL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "def get_discrete_state_key(state, grid_size=EXPLORATION_GRID_SIZE):\n",
        "    \"\"\"\n",
        "    For exploration bonus in continuous state spaces:\n",
        "    Round the state to a grid size and return a tuple as a dictionary key.\n",
        "    \"\"\"\n",
        "    return tuple((state / grid_size).astype(int))\n",
        "\n",
        "# policy networks\n",
        "class DiscretePolicyNetwork(nn.Module):\n",
        "    \"\"\"A simple MLP for discrete action spaces (e.g., CartPole).\"\"\"\n",
        "    def __init__(self, state_dim, action_dim, hidden_size=HIDDEN_SIZE):\n",
        "        super(DiscretePolicyNetwork, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, action_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)  # returns logits\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"\n",
        "        Samples an action according to the policy distribution.\n",
        "        Returns the action (int) and the log_prob of that action.\n",
        "        \"\"\"\n",
        "        logits = self.forward(state)\n",
        "        dist = torch.distributions.Categorical(logits=logits)\n",
        "        action = dist.sample()\n",
        "        return action.item(), dist.log_prob(action)\n",
        "\n",
        "\n",
        "class ContinuousPolicyNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple MLP policy for continuous action spaces (e.g., LunarLanderContinuous).\n",
        "    Outputs mean and log_std for each action dimension.\n",
        "    \"\"\"\n",
        "    def __init__(self, state_dim, action_dim, hidden_size=HIDDEN_SIZE):\n",
        "        super(ContinuousPolicyNetwork, self).__init__()\n",
        "        self.fc_mean = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, action_dim)\n",
        "        )\n",
        "        # We'll keep log_std as a trainable parameter\n",
        "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = self.fc_mean(x)\n",
        "        std = torch.exp(self.log_std)\n",
        "        return mean, std\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"\n",
        "        Samples a continuous action from a Normal distribution parameterized\n",
        "        by [mean, std] for each dimension.\n",
        "        Returns action (np.array) and log_prob (torch.tensor).\n",
        "        \"\"\"\n",
        "        mean, std = self.forward(state)\n",
        "        dist = torch.distributions.Normal(mean, std)\n",
        "        action_sample = dist.sample()   # shape: (1, action_dim) if batch_size=1\n",
        "\n",
        "        # Flatten to remove the batch dimension (so shape is (action_dim,))\n",
        "        action_np = action_sample.numpy().flatten()\n",
        "        log_prob = dist.log_prob(action_sample).sum(dim=-1)\n",
        "\n",
        "        return action_np, log_prob\n",
        "\n",
        "# reward shaping function\n",
        "def shape_reward(env_name, state, action, prev_action, raw_reward, state_visit_count):\n",
        "    \"\"\"\n",
        "    Incorporates:\n",
        "      1. Action-change penalty\n",
        "      2. State-dependent cost\n",
        "      3. Exploration bonus\n",
        "    \"\"\"\n",
        "    shaped_reward = raw_reward\n",
        "\n",
        "    # 1) Action-change penalty (smoothness)\n",
        "    if ENABLE_ACTION_CHANGE_PENALTY and prev_action is not None:\n",
        "        if isinstance(action, (int, np.integer)) and isinstance(prev_action, (int, np.integer)):\n",
        "            # Discrete: small penalty if action differs\n",
        "            if action != prev_action:\n",
        "                shaped_reward -= ACTION_CHANGE_PENALTY_WEIGHT\n",
        "        else:\n",
        "            # Continuous: penalize magnitude of difference\n",
        "            diff = np.linalg.norm(action - prev_action)\n",
        "            shaped_reward -= ACTION_CHANGE_PENALTY_WEIGHT * diff\n",
        "\n",
        "    # 2) State-dependent cost\n",
        "    if ENABLE_STATE_DEPENDENT_COST:\n",
        "        if \"CartPole\" in env_name:\n",
        "            # state[1] = x_dot, state[3] = theta_dot\n",
        "            cost = abs(state[1]) + abs(state[3])\n",
        "            shaped_reward -= STATE_DEPENDENT_COST_WEIGHT * cost\n",
        "        elif \"LunarLander\" in env_name:\n",
        "            # state = [x, y, x_dot, y_dot, theta, theta_dot, left_contact, right_contact]\n",
        "            # penalize large velocities\n",
        "            cost = abs(state[2]) + abs(state[3])\n",
        "            shaped_reward -= STATE_DEPENDENT_COST_WEIGHT * cost\n",
        "\n",
        "    # 3) Exploration bonus\n",
        "    if ENABLE_EXPLORATION_BONUS:\n",
        "        shaped_reward += EXPLORATION_BONUS_WEIGHT / (1.0 + state_visit_count)\n",
        "\n",
        "    return shaped_reward\n",
        "\n",
        "# episode trajectory\n",
        "def run_episode(env, policy, is_discrete=True):\n",
        "    \"\"\"\n",
        "    Runs a single episode using the given policy.\n",
        "    Returns lists of states, actions, log_probs, rewards (the entire episode).\n",
        "    \"\"\"\n",
        "    state, _ = env.reset(seed=SEED)  # new step API for resetting\n",
        "    done = False\n",
        "\n",
        "    states = []\n",
        "    actions = []\n",
        "    log_probs = []\n",
        "    rewards = []\n",
        "\n",
        "    prev_action = None\n",
        "\n",
        "    # For exploration bonus, maintain a dictionary of visit counts\n",
        "    visit_counts = defaultdict(int)\n",
        "\n",
        "    while not done:\n",
        "        state_t = torch.FloatTensor(state).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "        # Choose action\n",
        "        if is_discrete:\n",
        "            action, log_prob = policy.act(state_t)\n",
        "        else:\n",
        "            action, log_prob = policy.act(state_t)\n",
        "\n",
        "        # Discretize state for exploration counting\n",
        "        disc_state_key = get_discrete_state_key(state) if ENABLE_EXPLORATION_BONUS else None\n",
        "        if disc_state_key is not None:\n",
        "            visit_counts[disc_state_key] += 1\n",
        "            shaped_reward = shape_reward(\n",
        "                env.unwrapped.spec.id,\n",
        "                state,\n",
        "                action,\n",
        "                prev_action,\n",
        "                raw_reward=0.0,  # we'll add the environment reward later\n",
        "                state_visit_count=visit_counts[disc_state_key]\n",
        "            )\n",
        "        else:\n",
        "            shaped_reward = 0.0\n",
        "\n",
        "        # Step environment\n",
        "        next_state, raw_reward, done, _, info = env.step(action)\n",
        "\n",
        "        # Add the environment's native reward to the shaped reward\n",
        "        total_reward = shaped_reward + raw_reward\n",
        "\n",
        "        states.append(state)\n",
        "        actions.append(action)\n",
        "        log_probs.append(log_prob)\n",
        "        rewards.append(total_reward)\n",
        "\n",
        "        state = next_state\n",
        "        prev_action = action\n",
        "\n",
        "    return states, actions, log_probs, rewards\n",
        "\n",
        "# discount returns + update policy\n",
        "def update_policy(policy, optimizer, log_probs, rewards):\n",
        "    \"\"\"\n",
        "    Given the entire episode's log_probs and shaped rewards,\n",
        "    compute discounted returns and perform a policy gradient update.\n",
        "    \"\"\"\n",
        "    # Compute discounted returns\n",
        "    returns = []\n",
        "    G = 0\n",
        "    for r in reversed(rewards):\n",
        "        G = r + GAMMA * G\n",
        "        returns.insert(0, G)\n",
        "\n",
        "    returns = torch.tensor(returns, dtype=torch.float).to(DEVICE)\n",
        "    # Normalize returns for stability (optional but common)\n",
        "    returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
        "\n",
        "    # Accumulate policy loss\n",
        "    policy_loss = []\n",
        "    for log_prob, Gt in zip(log_probs, returns):\n",
        "        policy_loss.append(-log_prob * Gt)\n",
        "\n",
        "    policy_loss = torch.stack(policy_loss).sum()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    policy_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return policy_loss.item()\n",
        "\n",
        "# train loop\n",
        "def train_env(env_name=\"CartPole-v1\",\n",
        "              num_episodes=NUM_EPISODES,\n",
        "              lr=LR,\n",
        "              hidden_size=HIDDEN_SIZE):\n",
        "    \"\"\"\n",
        "    Generic training function. Automatically detects discrete vs continuous\n",
        "    action space and constructs the appropriate policy network.\n",
        "    \"\"\"\n",
        "    # Make environment with new_step_api to avoid old-step warnings\n",
        "    env = gym.make(env_name)\n",
        "    set_seed(SEED)\n",
        "\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    # If the environment's action space has no 'shape', it is discrete\n",
        "    is_discrete = (len(env.action_space.shape) == 0)\n",
        "\n",
        "    if is_discrete:\n",
        "        action_dim = env.action_space.n\n",
        "        policy = DiscretePolicyNetwork(state_dim, action_dim, hidden_size).to(DEVICE)\n",
        "    else:\n",
        "        action_dim = env.action_space.shape[0]\n",
        "        print(\"action_dim =\", action_dim)\n",
        "        policy = ContinuousPolicyNetwork(state_dim, action_dim, hidden_size).to(DEVICE)\n",
        "\n",
        "    optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
        "\n",
        "    episode_rewards = []\n",
        "    smoothed_rewards = []\n",
        "    losses = []\n",
        "\n",
        "    for ep in range(num_episodes):\n",
        "        states, actions, log_probs, rewards = run_episode(env, policy, is_discrete)\n",
        "        ep_reward = sum(rewards)\n",
        "        loss = update_policy(policy, optimizer, log_probs, rewards)\n",
        "\n",
        "        episode_rewards.append(ep_reward)\n",
        "        losses.append(loss)\n",
        "\n",
        "        # Smoothed (running average over last 10 episodes)\n",
        "        if len(episode_rewards) < 10:\n",
        "            smoothed_rewards.append(np.mean(episode_rewards[-len(episode_rewards):]))\n",
        "        else:\n",
        "            smoothed_rewards.append(np.mean(episode_rewards[-10:]))\n",
        "\n",
        "        if (ep + 1) % PRINT_EVERY == 0:\n",
        "            print(f\"Env: {env_name} | Episode: {ep+1} | Return: {ep_reward:.2f} | Avg(10): {smoothed_rewards[-1]:.2f}\")\n",
        "\n",
        "    env.close()\n",
        "    return episode_rewards, smoothed_rewards, losses\n"
      ],
      "metadata": {
        "id": "-EhcKxOhWrT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    shaping_configs = [\n",
        "        {\n",
        "            \"name\": \"baseline\",\n",
        "            \"ENABLE_ACTION_CHANGE_PENALTY\": False,\n",
        "            \"ACTION_CHANGE_PENALTY_WEIGHT\": 0.0,\n",
        "            \"ENABLE_STATE_DEPENDENT_COST\": False,\n",
        "            \"STATE_DEPENDENT_COST_WEIGHT\": 0.0,\n",
        "            \"ENABLE_EXPLORATION_BONUS\": False,\n",
        "            \"EXPLORATION_BONUS_WEIGHT\": 0.0\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"action_change_only\",\n",
        "            \"ENABLE_ACTION_CHANGE_PENALTY\": True,\n",
        "            \"ACTION_CHANGE_PENALTY_WEIGHT\": 0.001,\n",
        "            \"ENABLE_STATE_DEPENDENT_COST\": False,\n",
        "            \"STATE_DEPENDENT_COST_WEIGHT\": 0.0,\n",
        "            \"ENABLE_EXPLORATION_BONUS\": False,\n",
        "            \"EXPLORATION_BONUS_WEIGHT\": 0.0\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"state_dependent_only\",\n",
        "            \"ENABLE_ACTION_CHANGE_PENALTY\": False,\n",
        "            \"ACTION_CHANGE_PENALTY_WEIGHT\": 0.0,\n",
        "            \"ENABLE_STATE_DEPENDENT_COST\": True,\n",
        "            \"STATE_DEPENDENT_COST_WEIGHT\": 0.001,\n",
        "            \"ENABLE_EXPLORATION_BONUS\": False,\n",
        "            \"EXPLORATION_BONUS_WEIGHT\": 0.0\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"exploration_only\",\n",
        "            \"ENABLE_ACTION_CHANGE_PENALTY\": False,\n",
        "            \"ACTION_CHANGE_PENALTY_WEIGHT\": 0.0,\n",
        "            \"ENABLE_STATE_DEPENDENT_COST\": False,\n",
        "            \"STATE_DEPENDENT_COST_WEIGHT\": 0.0,\n",
        "            \"ENABLE_EXPLORATION_BONUS\": True,\n",
        "            \"EXPLORATION_BONUS_WEIGHT\": 0.01\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"combined\",\n",
        "            \"ENABLE_ACTION_CHANGE_PENALTY\": True,\n",
        "            \"ACTION_CHANGE_PENALTY_WEIGHT\": 0.001,\n",
        "            \"ENABLE_STATE_DEPENDENT_COST\": True,\n",
        "            \"STATE_DEPENDENT_COST_WEIGHT\": 0.001,\n",
        "            \"ENABLE_EXPLORATION_BONUS\": True,\n",
        "            \"EXPLORATION_BONUS_WEIGHT\": 0.01\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    N_EPISODES_CP = 300\n",
        "    N_EPISODES_LL = 300\n",
        "\n",
        "    results_cartpole = {}\n",
        "    results_lunar = {}\n",
        "\n",
        "    for config in shaping_configs:\n",
        "\n",
        "        globals()[\"ENABLE_ACTION_CHANGE_PENALTY\"] = config[\"ENABLE_ACTION_CHANGE_PENALTY\"]\n",
        "        globals()[\"ACTION_CHANGE_PENALTY_WEIGHT\"] = config[\"ACTION_CHANGE_PENALTY_WEIGHT\"]\n",
        "        globals()[\"ENABLE_STATE_DEPENDENT_COST\"] = config[\"ENABLE_STATE_DEPENDENT_COST\"]\n",
        "        globals()[\"STATE_DEPENDENT_COST_WEIGHT\"] = config[\"STATE_DEPENDENT_COST_WEIGHT\"]\n",
        "        globals()[\"ENABLE_EXPLORATION_BONUS\"] = config[\"ENABLE_EXPLORATION_BONUS\"]\n",
        "        globals()[\"EXPLORATION_BONUS_WEIGHT\"] = config[\"EXPLORATION_BONUS_WEIGHT\"]\n",
        "\n",
        "        print(f\"\\n==== Running CartPole with config: {config['name']} ====\")\n",
        "        cp_returns, cp_smoothed, cp_losses = train_env(\n",
        "            env_name=\"CartPole-v1\",\n",
        "            num_episodes=N_EPISODES_CP,\n",
        "            lr=LR,\n",
        "            hidden_size=HIDDEN_SIZE\n",
        "        )\n",
        "        results_cartpole[config[\"name\"]] = (cp_returns, cp_smoothed, cp_losses)\n",
        "\n",
        "        print(f\"\\n==== Running LunarLander with config: {config['name']} ====\")\n",
        "        ll_returns, ll_smoothed, ll_losses = train_env(\n",
        "            env_name=\"LunarLanderContinuous-v3\",\n",
        "            num_episodes=N_EPISODES_LL,\n",
        "            lr=LR,\n",
        "            hidden_size=HIDDEN_SIZE\n",
        "        )\n",
        "        results_lunar[config[\"name\"]] = (ll_returns, ll_smoothed, ll_losses)\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    episodes_cp = range(N_EPISODES_CP)\n",
        "\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.title(\"CartPole - Episode Returns (Multiple Shaping Configs)\")\n",
        "    for cfg_name, (cp_returns, cp_smoothed, cp_losses) in results_cartpole.items():\n",
        "        plt.plot(episodes_cp, cp_returns, label=f\"{cfg_name} (raw)\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Episode Return (Shaped)\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.title(\"CartPole - Smoothed Returns\")\n",
        "    for cfg_name, (cp_returns, cp_smoothed, cp_losses) in results_cartpole.items():\n",
        "        plt.plot(episodes_cp, cp_smoothed, label=f\"{cfg_name} (smoothed)\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Smoothed Return\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.title(\"CartPole - Policy Loss\")\n",
        "    for cfg_name, (cp_returns, cp_smoothed, cp_losses) in results_cartpole.items():\n",
        "        plt.plot(episodes_cp, cp_losses, label=f\"{cfg_name} (loss)\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # -- LUNARLANDER PLOTS --\n",
        "    episodes_ll = range(N_EPISODES_LL)\n",
        "\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.title(\"LunarLander - Episode Returns (Multiple Shaping Configs)\")\n",
        "    for cfg_name, (ll_returns, ll_smoothed, ll_losses) in results_lunar.items():\n",
        "        plt.plot(episodes_ll, ll_returns, label=f\"{cfg_name} (raw)\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Episode Return (Shaped)\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.title(\"LunarLander - Smoothed Returns\")\n",
        "    for cfg_name, (ll_returns, ll_smoothed, ll_losses) in results_lunar.items():\n",
        "        plt.plot(episodes_ll, ll_smoothed, label=f\"{cfg_name} (smoothed)\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Smoothed Return\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.title(\"LunarLander - Policy Loss\")\n",
        "    for cfg_name, (ll_returns, ll_smoothed, ll_losses) in results_lunar.items():\n",
        "        plt.plot(episodes_ll, ll_losses, label=f\"{cfg_name} (loss)\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zi65BlZzsKUV",
        "outputId": "a467cda7-b012-4c7c-c413-27cac133ab6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== Running CartPole with config: baseline ====\n",
            "Env: CartPole-v1 | Episode: 50 | Return: 18.00 | Avg(10): 20.80\n",
            "Env: CartPole-v1 | Episode: 100 | Return: 38.00 | Avg(10): 27.30\n",
            "Env: CartPole-v1 | Episode: 150 | Return: 60.00 | Avg(10): 90.50\n",
            "Env: CartPole-v1 | Episode: 200 | Return: 166.00 | Avg(10): 198.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install swig\n",
        "!pip install \"gymnasium[box2d]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBX-BXUUxj0l",
        "outputId": "26bac8d3-c671-47ae-8c7f-b48fb512d476"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting swig\n",
            "  Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/1.9 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.3.0\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.3.0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp311-cp311-linux_x86_64.whl size=2379445 sha256=de2f728da96d2ad64442cabd09c0e21e974c83ba5c1e6187a0bbb3862d233a12\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/f1/0c/d56f4a2bdd12bae0a0693ec33f2f0daadb5eb9753c78fa5308\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y swig build-essential python3-dev\n",
        "!pip install --upgrade pip setuptools wheel\n",
        "!pip install --no-deps \"gym==0.25.2\"\n",
        "!pip install \"box2d-py==2.3.5\" \"pygame>=2.3.0\"\n",
        "!pip install \"numpy>=1.18.0\" \"cloudpickle>=1.2.0\" \"gym_notices>=0.0.4\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 825
        },
        "id": "bI2bHn1DeBxh",
        "outputId": "639ec230-1a9a-498a-e46f-30b4b4975206"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "swig is already the newest version (4.0.2-1ubuntu1).\n",
            "python3-dev is already the newest version (3.10.6-1~22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (78.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (0.45.1)\n",
            "Requirement already satisfied: gym==0.25.2 in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Collecting box2d-py==2.3.5\n",
            "  Using cached box2d-py-2.3.5.tar.gz (374 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.3.0 in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp311-cp311-linux_x86_64.whl size=2351251 sha256=953f4e6b5e2de1416eeb7ffadac4b39917ea39922c1b4f3d61dcef31ff596d52\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/f1/0c/d56f4a2bdd12bae0a0693ec33f2f0daadb5eb9753c78fa5308\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py\n",
            "  Attempting uninstall: box2d-py\n",
            "    Found existing installation: box2d-py 2.3.8\n",
            "    Uninstalling box2d-py-2.3.8:\n",
            "      Successfully uninstalled box2d-py-2.3.8\n",
            "Successfully installed box2d-py-2.3.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "Box2D"
                ]
              },
              "id": "f0816a9151b645428cfa24cc444b67b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (3.1.1)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (0.0.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "env = gym.make(\"LunarLanderContinuous-v2\")\n",
        "obs, info = env.reset()\n",
        "action = env.action_space.sample()\n",
        "obs, reward, done, truncated, info = env.step(action)\n",
        "print(\"Success:\", obs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRF8BL1Ue7T-",
        "outputId": "e42e7b39-237d-44dc-f8a2-8885ffffae10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:6 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,381 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,684 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,045 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,784 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,049 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,538 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [55.7 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,892 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [47.7 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,239 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,737 kB]\n",
            "Fetched 29.9 MB in 4s (8,518 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "python3.11-dev is already the newest version (3.11.11-1+jammy1).\n",
            "The following additional packages will be installed:\n",
            "  swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 38 not upgraded.\n",
            "Need to get 1,116 kB of archives.\n",
            "After this operation, 5,542 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n",
            "Fetched 1,116 kB in 1s (1,574 kB/s)\n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 126209 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (78.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (0.45.1)\n",
            "Collecting box2d-py\n",
            "  Using cached box2d-py-2.3.8.tar.gz (374 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.8-cp311-cp311-linux_x86_64.whl size=2351249 sha256=8ad3d72e908fc162b24889204ac92011b11934e4c7f074708b4e72d91481751a\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/95/02/4cb5adc9f6dcaeb9639c2271f630a66ab4440102414804c45c\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fUY5d8_act4y"
      }
    }
  ]
}